{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44a1e795",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1449aedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db6257d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import RegexTokenizer\n",
    "tokenizer = RegexTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "559e87ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tokenizer_equivalence(text, dataset, vocab_size, chunk_size=100000, verbose=False):\n",
    "    # Initialize two tokenizers with the same configuration\n",
    "    tokenizer1 = RegexTokenizer()\n",
    "    tokenizer2 = RegexTokenizer()\n",
    "\n",
    "    # Train using the `train` method (single text input)\n",
    "    tokenizer1.train(text, vocab_size, verbose)\n",
    "\n",
    "    # Train using the `train_dataset` method (dataset input in chunks)\n",
    "    tokenizer2.train_dataset(dataset, vocab_size, chunk_size, verbose)\n",
    "\n",
    "    # Compare merges\n",
    "    merges_same = tokenizer1.merges == tokenizer2.merges\n",
    "    print(\"Merges Match:\", merges_same)\n",
    "\n",
    "    # Compare vocabularies\n",
    "    vocab_same = tokenizer1.vocab == tokenizer2.vocab\n",
    "    print(\"Vocabularies Match:\", vocab_same)\n",
    "\n",
    "    # Print test result\n",
    "    if merges_same and vocab_same:\n",
    "        print(\"Test Passed: Both methods produce identical results.\")\n",
    "    else:\n",
    "        print(\"Test Failed: Differences found between methods.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbe7e9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 25439.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data chunk is ---> Sample text data for tokenizer testing. <------\n",
      "Data chunk is ---> Sample text data for tokenizer testing. <------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 6929.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merges Match: True\n",
      "Vocabularies Match: True\n",
      "Test Passed: Both methods produce identical results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample text input\n",
    "text = \"Sample text data for tokenizer testing. Sample text data for tokenizer testing.\"\n",
    "\n",
    "# Example dataset format for `train_dataset`\n",
    "# Simulate a dataset with similar content, e.g., using a list of dictionaries\n",
    "dataset = [{'text': \"Sample text data for tokenizer testing.\"} for _ in range(2)]\n",
    "\n",
    "vocab_size = 280\n",
    "\n",
    "test_tokenizer_equivalence(text, dataset, vocab_size, chunk_size=1, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70809ead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f64f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3174bf86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 6016/9744 [09:52<06:29,  9.58it/s]"
     ]
    }
   ],
   "source": [
    "file_path = 'data/taylorswift.txt'\n",
    "\n",
    "# Load the file content\n",
    "with open(file_path, 'r') as file:\n",
    "    file_content = file.read()\n",
    "\n",
    "# Set up a larger dataset by simulating multiple samples from the file content\n",
    "# For simplicity, let's split by paragraphs or sentences (here assuming each line is a sentence)\n",
    "dataset = [{'text': line} for line in file_content.splitlines() if line.strip()]\n",
    "\n",
    "# Set vocabulary size and chunk size\n",
    "vocab_size = 10000  # Adjust vocab size as needed for larger tests\n",
    "chunk_size = 5     # Smaller chunk size for more granular testing\n",
    "\n",
    "# Run the test function\n",
    "test_tokenizer_equivalence(file_content, dataset, vocab_size, chunk_size, verbose=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc51c362",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = {\n",
    "    '<|endoftext|>': 100257,\n",
    "    '<|fim_prefix|>': 100258,\n",
    "    '<|fim_middle|>': 100259,\n",
    "    '<|fim_suffix|>': 100260,\n",
    "    '<|endofprompt|>': 100276\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f3ab53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce40b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.encode(\"hello world\") # string -> tokens\n",
    "#tokenizer.decode([1000, 2000, 3000]) # tokens -> string\n",
    "#tokenizer.save(\"tok32k\") # writes tok32k.model and tok32k.vocab\n",
    "#tokenizer.load(\"tok32k.model\") # loads the model back from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927bc2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize dataset and save for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e828438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load your dataset\n",
    "dataset = load_dataset(\"your_dataset_name\", split=\"train\")\n",
    "\n",
    "# Define special tokens\n",
    "special_tokens = {\"bos\": \"<|bos|>\", \"eos\": \"<|endoftext|>\", \"pad\": \"<|pad|>\", \"unk\": \"<|unk|>\"}\n",
    "special_tokens_ids = {\"<|bos|>\":10001,  \"<|endoftext|>\":10002, \"<|pad|>\": 10003, \"unk\": 10004}\n",
    "\n",
    "# Preprocess the text to add special tokens\n",
    "def add_special_tokens(example):\n",
    "    example[\"text\"] = f\"{special_tokens['bos']} {example['text']} {special_tokens['eos']}\"\n",
    "    return example\n",
    "\n",
    "# Apply the special tokens to the text in the dataset\n",
    "dataset = dataset.map(add_special_tokens)\n",
    "\n",
    "\n",
    "# Tokenize the dataset and save tokenized ids\n",
    "def tokenize_function(example):\n",
    "    tokens = tokenizer.encode(example[\"text\"])\n",
    "    return {\"input_ids\": tokens.ids}\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset.save_to_disk(\"tokenized_dataset\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
